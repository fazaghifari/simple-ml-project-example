{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c459c3-beaa-4811-a74a-6cf3c3d337f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import joblib\n",
    "import time\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a897f2-e73c-4845-ae1b-ce8a368827d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, plot_roc_curve, make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import lightgbm as lgb\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a03a2-ac38-46e0-8ec2-815ed8c7d983",
   "metadata": {},
   "source": [
    "# Load and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e043c667-e114-49ae-af8e-8084ef28f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df, x_col, y_col):\n",
    "    x_col = ['id']+x_col\n",
    "    y_col = ['id']+y_col\n",
    "    return df[x_col], df[y_col]\n",
    "\n",
    "\n",
    "def get_stratify_col(y, stratify_col):\n",
    "    if stratify_col is None:\n",
    "        stratification = None\n",
    "    else:\n",
    "        stratification = y[stratify_col]\n",
    "    \n",
    "    return stratification\n",
    "\n",
    "\n",
    "def run_split_data(x, y, stratify_col=None, TEST_SIZE=0.2):\n",
    "    \n",
    "    strat_train = get_stratify_col(y, stratify_col)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                       stratify = strat_train,\n",
    "                                       test_size= TEST_SIZE*2,\n",
    "                                       random_state= 42)\n",
    "    \n",
    "    strat_test = get_stratify_col(y_test, stratify_col)\n",
    "    x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test,\n",
    "                                       stratify = strat_test,\n",
    "                                       test_size= 0.5,\n",
    "                                       random_state= 42)\n",
    "    \n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfeb8d7-0fc7-4f59-8f3c-b138fbf748d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_load(file_loc):\n",
    "    df = pd.read_csv(file_loc)\n",
    "    x_all, y_all = split_xy(df, ['comment_text'], ['toxic'])\n",
    "    x_train, y_train,x_valid, y_valid,x_test, y_test = run_split_data(x_all, y_all, 'toxic', 0.2)\n",
    "    joblib.dump(x_train, \"../output/x_train.pkl\")\n",
    "    joblib.dump(y_train, \"../output/y_train.pkl\")\n",
    "    joblib.dump(x_valid, \"../output/x_valid.pkl\")\n",
    "    joblib.dump(y_valid, \"../output/y_valid.pkl\")\n",
    "    joblib.dump(x_test, \"../output/x_test.pkl\")\n",
    "    joblib.dump(y_test, \"../output/y_test.pkl\")\n",
    "    \n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc1ef98-214b-426d-addd-edb21caa46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = '../data/comments_data.csv'\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = main_load(file_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018251a-b54d-423b-b871-d8701866c3af",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cabd67a3-d2cd-4598-8317-0acfb1259c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_char(df_in):\n",
    "    df = df_in.copy()  # Avoid modifying the main dataframe\n",
    "    df['comment_text'] = df['comment_text'].str.lower()\n",
    "    return df\n",
    "\n",
    "def phrase_decontraction(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def decontract(df_in):\n",
    "    df = df_in.copy()  # Avoid modifying the main dataframe\n",
    "    df['comment_text'] = df['comment_text'].apply(phrase_decontraction)\n",
    "    return df\n",
    "\n",
    "def remove_numbers(df_in):\n",
    "    df = df_in.copy()  # Avoid modifying the main dataframe\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x: ''.join(string for string in x if not string.isdigit()))\n",
    "    return df\n",
    "\n",
    "def remove_punc(df_in):\n",
    "    df = df_in.copy()  # Avoid modifying the main dataframe\n",
    "    df['comment_text'] = df['comment_text'].str.replace(f'[{punctuation}]', ' ', regex=True )\n",
    "    return df\n",
    "\n",
    "def remove_whitespace(df_in):\n",
    "    df = df_in.copy()  # Avoid modifying the main dataframe\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x: \" \".join(x.split()))\n",
    "    return df\n",
    "\n",
    "def remove_stop(df_in, eng_stopwords):\n",
    "    df = df_in.copy()  # Avoid modifying the main dataframe\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x: \" \".join([word for word in nltk.word_tokenize(x) if word not in eng_stopwords]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a6ea43-7e0b-40c8-9fce-d4085472c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_in):\n",
    "    eng_stopwords = stopwords.words('english')\n",
    "    df = df_in.copy()\n",
    "    df = lowercase_char(df)\n",
    "    df = decontract(df)\n",
    "    df = remove_numbers(df)\n",
    "    df = remove_punc(df)\n",
    "    df = remove_whitespace(df)\n",
    "    df = remove_stop(df, eng_stopwords)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85477927-937c-41d3-84ed-2c9d0fe0e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_prep(x_train,x_valid,x_test):\n",
    "    x_list = [x_train,x_valid,x_test]\n",
    "\n",
    "    x_preprocessed = []\n",
    "    for x in tqdm(x_list):\n",
    "        temp = preprocess(x)\n",
    "        x_preprocessed.append(temp)\n",
    "\n",
    "    name = ['train','valid','test']\n",
    "    for i,x in tqdm(enumerate(x_preprocessed)):\n",
    "        joblib.dump(x, f\"../output/x_{name[i]}_preprocessed.pkl\")\n",
    "    \n",
    "    return x_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d34ee813-732e-45ab-a8b6-e25b9622428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:25<00:00, 28.34s/it]\n",
      "3it [00:00, 10.53it/s]\n"
     ]
    }
   ],
   "source": [
    "x_preprocessed_list = main_prep(x_train,x_valid,x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb8607-8bdf-4566-8f65-9a67cf4ab3c8",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "495fd56b-a0b5-45a6-afa1-20f7c5a8f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tfidf(df_in, vectorizer=None):\n",
    "    df = df_in.copy()\n",
    "    if vectorizer is None:  # fit to train data\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            analyzer='word',\n",
    "            stop_words='english',\n",
    "            min_df = 1e-2\n",
    "        )\n",
    "        vectorized = vectorizer.fit_transform(df['comment_text'])\n",
    "        joblib.dump(vectorizer, \"../output/vectorizer.pkl\")\n",
    "    else:\n",
    "        vectorized = vectorizer.transform(df['comment_text'])\n",
    "    \n",
    "    vectorized_df = pd.DataFrame(vectorized.toarray(), \n",
    "                                 columns=vectorizer.get_feature_names(), \n",
    "                                 index = df.index)\n",
    "    df_non_sentence = df.drop(['comment_text'],axis=1)\n",
    "    df_final = pd.concat([vectorized_df, df_non_sentence],axis=1)\n",
    "    return df_final, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af9e4188-c9a8-48a9-b2ed-0d5b65f3e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_feat(x_preprocessed_list):\n",
    "    x_train_preprocessed, x_valid_preprocessed, x_test_preprocessed = x_preprocessed_list\n",
    "    df_train_vect, vectorizer = vectorize_tfidf(x_train_preprocessed)\n",
    "    df_valid_vect, _ = vectorize_tfidf(x_valid_preprocessed, vectorizer)\n",
    "    df_test_vect, _ = vectorize_tfidf(x_test_preprocessed, vectorizer)\n",
    "    joblib.dump(df_train_vect, f\"../output/x_train_vect.pkl\")\n",
    "    joblib.dump(df_valid_vect, f\"../output/x_valid_vect.pkl\")\n",
    "    joblib.dump(df_test_vect, f\"../output/x_test_vect.pkl\")\n",
    "    \n",
    "    return df_train_vect, df_valid_vect, df_test_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3898ed27-b5a3-45b2-a9a9-6399123a4ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ghifa\\documents\\pacmann\\simple-ml-project-example\\projectenv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\users\\ghifa\\documents\\pacmann\\simple-ml-project-example\\projectenv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\users\\ghifa\\documents\\pacmann\\simple-ml-project-example\\projectenv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train_vect, x_valid_vect, x_test_vect = main_feat(x_preprocessed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b5374-aa13-4c86-a0d4-7de96590cf87",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ee078c2-0b7c-4c38-bdd8-ed100520ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_logreg(class_weight = None):\n",
    "    \"\"\"\n",
    "    Function for initiating Logistic Regression Model\n",
    "    \"\"\"\n",
    "    param_dist = {'C' : [0.25, 0.5, 1]}\n",
    "    base_model = LogisticRegression(random_state=42, solver='liblinear', class_weight=class_weight)\n",
    "    \n",
    "    return param_dist, base_model\n",
    "\n",
    "def model_rf(class_weight = None):\n",
    "    \"\"\"\n",
    "    Function for initiating Random Forest Model\n",
    "    \"\"\"\n",
    "    param_dist = {'n_estimators' : [25, 50, 100]}\n",
    "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=class_weight)\n",
    "    \n",
    "    return param_dist, base_model\n",
    "\n",
    "def model_lgb(class_weight = None):\n",
    "    \"\"\"\n",
    "    Function for initiating LightGBM Model\n",
    "    \"\"\"\n",
    "    param_dist = {'n_estimators' : [25, 50, 100], 'boosting_type':['gbdt', 'goss']}\n",
    "    base_model = lgb.LGBMClassifier(random_state=42, n_jobs=-1, class_weight=class_weight)\n",
    "    \n",
    "    return param_dist, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "328b4f28-b863-4c44-974a-158e0d742110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_cv(model, param, scoring, n_iter, x, y, verbosity=0):\n",
    "    \"\"\"\n",
    "    Just a function to run the hyperparameter search\n",
    "    \"\"\"\n",
    "    random_fit = RandomizedSearchCV(estimator = model, \n",
    "                                    param_distributions = param, \n",
    "                                    scoring = scoring, \n",
    "                                    n_iter = n_iter, \n",
    "                                    cv = 5, \n",
    "                                    random_state = 42, \n",
    "                                    verbose = verbosity)\n",
    "    random_fit.fit(x, y)\n",
    "    return random_fit\n",
    "\n",
    "def calibrate_classifier(model, x_valid, y_valid):\n",
    "    model_calibrated = CalibratedClassifierCV(model, cv='prefit')\n",
    "    model_calibrated.fit(x_valid, y_valid)\n",
    "    \n",
    "    return model_calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e204888-4636-4ec6-9c09-7e6b98972170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_threshold(model, x_valid, y_valid, scorer):\n",
    "    \"\"\"\n",
    "    Function for threshold adjustment\n",
    "    \n",
    "    Args:\n",
    "        - model(callable): Sklearn model\n",
    "        - x_valid(DataFrame):\n",
    "        - y_valid(DataFrame):\n",
    "        - scorer(callable): Sklearn scorer function, for example: f1_score\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0,1,101)\n",
    "    proba = model.predict_proba(x_valid)[:, 1]\n",
    "    proba = pd.DataFrame(proba)\n",
    "    proba.columns = ['probability']\n",
    "    score = []\n",
    "    for threshold_value in thresholds:\n",
    "        proba['prediction'] = np.where( proba['probability'] > threshold_value, 1, 0)\n",
    "        metric_score = scorer(proba['prediction'], y_valid, average='macro')\n",
    "        score.append(metric_score)\n",
    "    metric_score = pd.DataFrame([thresholds,score]).T\n",
    "    metric_score.columns = ['threshold','metric_score']\n",
    "    best_score = (metric_score['metric_score'] == metric_score['metric_score'].max())\n",
    "    best_threshold = metric_score[best_score]['threshold']\n",
    "    \n",
    "    return metric_score[\"metric_score\"].max(), best_threshold.values[0]\n",
    "\n",
    "def select_model(train_log_dict):\n",
    "    max_score = max(train_log_dict['model_score'])\n",
    "    max_index = train_log_dict['model_score'].index(max_score)\n",
    "    best_model = train_log_dict['model_fit'][max_index]\n",
    "    best_report = train_log_dict['model_report'][max_index]\n",
    "    best_threshold = train_log_dict['threshold'][max_index]\n",
    "    name = train_log_dict['model_name'][max_index]\n",
    "\n",
    "    return best_model, best_report, best_threshold, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "356c5e51-a1cc-4a9b-9c04-3948dd719aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif_report(model_obj, x_test, y_test, best_threshold=None, calc_auc=True):\n",
    "    code2rel = {'0': 'Non-Toxic', '1': 'Toxic'}\n",
    "    \n",
    "    if best_threshold is None:\n",
    "        pred = model_obj.predict(x_test)\n",
    "    else:\n",
    "        proba = model_obj.predict_proba(x_test)[:, 1]\n",
    "        pred = np.where(proba > best_threshold, 1, 0)\n",
    "\n",
    "    res = classification_report(\n",
    "        y_test, pred, output_dict=True, zero_division=0)\n",
    "    res = pd.DataFrame(res).rename(columns=code2rel).T\n",
    "\n",
    "    if calc_auc:\n",
    "        proba = model_obj.predict_proba(x_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, proba)\n",
    "\n",
    "        print(\n",
    "            f\"AUC score: {auc_score}, F1-Macro: {res['f1-score']['macro avg']}\")\n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3c070d1-5d52-449d-8172-39a3877a6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train, model, model_param, scoring='f1', n_iter=3, verbosity=3):\n",
    "    \"\"\"\n",
    "    Fit model\n",
    "    \n",
    "    Args:\n",
    "        - model(callable): sklearn model\n",
    "        - model_param(dict): sklearn's RandomizedSearchCV params_distribution\n",
    "    \n",
    "    Return:\n",
    "        - model_fitted(callable): model with optimum hyperparams\n",
    "    \"\"\"\n",
    "    model_fitted = random_search_cv(model, model_param, \n",
    "                                    scoring, \n",
    "                                    n_iter, \n",
    "                                    x_train, y_train, \n",
    "                                    verbosity)\n",
    "    print(\n",
    "        f'Model: {model_fitted.best_estimator_}, {scoring}: {model_fitted.best_score_}')\n",
    "    \n",
    "    return model_fitted\n",
    "\n",
    "def validate(x_valid, y_valid, model_fitted, tune = True):\n",
    "    \"\"\"\n",
    "    Validate model\n",
    "\n",
    "    Args:\n",
    "        - x_valid(DataFrame): Validation independent variables\n",
    "        - y_valid(DataFrame): Validation Dependent variables\n",
    "        - model_fitted(callable): Sklearn / imblearn fitted model\n",
    "    \"\"\"\n",
    "    code2rel = {'0': 'Non-Toxic', '1': 'Toxic'}\n",
    "\n",
    "    # Calibrate Classifier\n",
    "    model_calibrated = CalibratedClassifierCV(base_estimator=model_fitted,\n",
    "                                              cv=\"prefit\")\n",
    "    model_calibrated.fit(x_valid, y_valid)\n",
    "    \n",
    "    if tune:\n",
    "        metric_score, best_threshold = tune_threshold(model_calibrated,\n",
    "                                                      x_valid,\n",
    "                                                      y_valid,\n",
    "                                                      f1_score)\n",
    "        \n",
    "        print(f'Best threshold is: {best_threshold}, with score: {metric_score}')\n",
    "        pred_model, report_model = classif_report(model_calibrated,\n",
    "                                                  x_valid,\n",
    "                                                  y_valid,\n",
    "                                                  best_threshold,\n",
    "                                                  True)\n",
    "    else:\n",
    "        # Report default\n",
    "        best_threshold = None\n",
    "        pred_model, report_model = classif_report(\n",
    "            model_calibrated, x_valid, y_valid, True)\n",
    "\n",
    "    return report_model, model_calibrated, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c55d01c-67a7-40e1-8776-542f708253a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(x_train, y_train, x_valid, y_valid):\n",
    "    \n",
    "    x_train = x_train.drop(columns='id')\n",
    "    y_train = y_train.drop(columns='id')\n",
    "    x_valid = x_valid.drop(columns='id')\n",
    "    y_valid = y_valid.drop(columns='id')\n",
    "    \n",
    "    y_train = y_train.values.ravel()\n",
    "    y_valid = y_valid.values.ravel()\n",
    "\n",
    "    # Add class weight\n",
    "    class_weight = compute_class_weight(class_weight = 'balanced', \n",
    "                                        classes = np.unique(y_train), \n",
    "                                        y = y_train)\n",
    "    class_weights = dict(zip(np.unique(y_train), class_weight))\n",
    "    \n",
    "    # Initiate models\n",
    "    logreg = model_logreg\n",
    "    rf = model_rf\n",
    "    lgb = model_lgb\n",
    "    \n",
    "    # Initiate logs\n",
    "    train_log_dict = {'model': [logreg, rf, lgb],\n",
    "                      'model_name': [],\n",
    "                      'model_fit': [],\n",
    "                      'model_report': [],\n",
    "                      'model_score': [],\n",
    "                      'threshold': [],\n",
    "                      'fit_time': []}\n",
    "\n",
    "\n",
    "    # Try Each models\n",
    "    for model in train_log_dict['model']:\n",
    "        param_model, base_model = model(class_weights)\n",
    "        train_log_dict['model_name'].append(base_model.__class__.__name__)\n",
    "        print(f'Fitting {base_model.__class__.__name__}')\n",
    "\n",
    "        # Train\n",
    "        t0 = time.time()\n",
    "        scoring = make_scorer(f1_score,average='macro')\n",
    "        fitted_model = fit(\n",
    "            x_train, y_train, base_model, param_model, scoring=scoring)\n",
    "        elapsed_time = time.time() - t0\n",
    "        print(f'elapsed time: {elapsed_time} s \\n')\n",
    "        train_log_dict['fit_time'].append(elapsed_time)\n",
    "\n",
    "        # Validate\n",
    "        report, calibrated_model, best_threshold = validate(\n",
    "            x_valid, y_valid, fitted_model)\n",
    "        train_log_dict['model_fit'].append(calibrated_model)\n",
    "        train_log_dict['threshold'].append(best_threshold)\n",
    "        train_log_dict['model_report'].append(report)\n",
    "        train_log_dict['model_score'].append(report['f1-score']['macro avg'])\n",
    "\n",
    "    best_model, best_report, best_threshold, name = select_model(\n",
    "        train_log_dict)\n",
    "    print(\n",
    "        f\"Model: {name}, Score: {best_report['f1-score']['macro avg']}\")\n",
    "    joblib.dump(best_model, '../output/mantab_model.pkl')\n",
    "    joblib.dump(best_threshold, '../output/threshold.pkl')\n",
    "    joblib.dump(train_log_dict, '../output/train_log.pkl')\n",
    "    print(f'\\n {best_report}')\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da4d9251-e02c-42f0-9ae9-abc213170010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LogisticRegression\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END ............................C=0.25;, score=0.640 total time=   0.7s\n",
      "[CV 2/5] END ............................C=0.25;, score=0.647 total time=   0.7s\n",
      "[CV 3/5] END ............................C=0.25;, score=0.639 total time=   0.7s\n",
      "[CV 4/5] END ............................C=0.25;, score=0.642 total time=   0.7s\n",
      "[CV 5/5] END ............................C=0.25;, score=0.642 total time=   0.7s\n",
      "[CV 1/5] END .............................C=0.5;, score=0.638 total time=   0.7s\n",
      "[CV 2/5] END .............................C=0.5;, score=0.646 total time=   0.7s\n",
      "[CV 3/5] END .............................C=0.5;, score=0.638 total time=   0.7s\n",
      "[CV 4/5] END .............................C=0.5;, score=0.641 total time=   0.7s\n",
      "[CV 5/5] END .............................C=0.5;, score=0.640 total time=   0.7s\n",
      "[CV 1/5] END ...............................C=1;, score=0.637 total time=   0.7s\n",
      "[CV 2/5] END ...............................C=1;, score=0.646 total time=   0.7s\n",
      "[CV 3/5] END ...............................C=1;, score=0.638 total time=   0.7s\n",
      "[CV 4/5] END ...............................C=1;, score=0.640 total time=   0.7s\n",
      "[CV 5/5] END ...............................C=1;, score=0.639 total time=   0.8s\n",
      "Model: LogisticRegression(C=0.25,\n",
      "                   class_weight={0: 0.5524624642168604, 1: 5.265311804008909},\n",
      "                   random_state=42, solver='liblinear'), make_scorer(f1_score, average=macro): 0.6421089289103569\n",
      "elapsed time: 13.760010242462158 s \n",
      "\n",
      "Best threshold is: 0.28, with score: 0.7274590201066383\n",
      "AUC score: 0.8707963639565767, F1-Macro: 0.7274590201066383\n",
      "Fitting RandomForestClassifier\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END ...................n_estimators=25;, score=0.678 total time=  14.1s\n",
      "[CV 2/5] END ...................n_estimators=25;, score=0.693 total time=  13.2s\n",
      "[CV 3/5] END ...................n_estimators=25;, score=0.684 total time=  12.6s\n",
      "[CV 4/5] END ...................n_estimators=25;, score=0.686 total time=  12.9s\n",
      "[CV 5/5] END ...................n_estimators=25;, score=0.688 total time=  13.0s\n",
      "[CV 1/5] END ...................n_estimators=50;, score=0.680 total time=  23.8s\n",
      "[CV 2/5] END ...................n_estimators=50;, score=0.692 total time=  23.6s\n",
      "[CV 3/5] END ...................n_estimators=50;, score=0.685 total time=  23.2s\n",
      "[CV 4/5] END ...................n_estimators=50;, score=0.685 total time=  23.3s\n",
      "[CV 5/5] END ...................n_estimators=50;, score=0.687 total time=  23.7s\n",
      "[CV 1/5] END ..................n_estimators=100;, score=0.679 total time=  49.8s\n",
      "[CV 2/5] END ..................n_estimators=100;, score=0.693 total time=  45.6s\n",
      "[CV 3/5] END ..................n_estimators=100;, score=0.686 total time=  45.9s\n",
      "[CV 4/5] END ..................n_estimators=100;, score=0.686 total time=  46.3s\n",
      "[CV 5/5] END ..................n_estimators=100;, score=0.689 total time=  47.0s\n",
      "Model: RandomForestClassifier(class_weight={0: 0.5524624642168604,\n",
      "                                     1: 5.265311804008909},\n",
      "                       n_jobs=-1, random_state=42), make_scorer(f1_score, average=macro): 0.6865090160448547\n",
      "elapsed time: 480.77740478515625 s \n",
      "\n",
      "Best threshold is: 0.27, with score: 0.6877470733863128\n",
      "AUC score: 0.8392941864478894, F1-Macro: 0.6877470733863128\n",
      "Fitting LGBMClassifier\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END boosting_type=gbdt, n_estimators=25;, score=0.593 total time=   2.1s\n",
      "[CV 2/5] END boosting_type=gbdt, n_estimators=25;, score=0.607 total time=   2.2s\n",
      "[CV 3/5] END boosting_type=gbdt, n_estimators=25;, score=0.602 total time=   2.1s\n",
      "[CV 4/5] END boosting_type=gbdt, n_estimators=25;, score=0.602 total time=   2.1s\n",
      "[CV 5/5] END boosting_type=gbdt, n_estimators=25;, score=0.601 total time=   2.3s\n",
      "[CV 1/5] END boosting_type=gbdt, n_estimators=50;, score=0.616 total time=   3.5s\n",
      "[CV 2/5] END boosting_type=gbdt, n_estimators=50;, score=0.625 total time=   3.4s\n",
      "[CV 3/5] END boosting_type=gbdt, n_estimators=50;, score=0.617 total time=   3.1s\n",
      "[CV 4/5] END boosting_type=gbdt, n_estimators=50;, score=0.618 total time=   3.2s\n",
      "[CV 5/5] END boosting_type=gbdt, n_estimators=50;, score=0.619 total time=   2.8s\n",
      "[CV 1/5] END boosting_type=goss, n_estimators=100;, score=0.637 total time=   4.1s\n",
      "[CV 2/5] END boosting_type=goss, n_estimators=100;, score=0.643 total time=   4.1s\n",
      "[CV 3/5] END boosting_type=goss, n_estimators=100;, score=0.636 total time=   4.1s\n",
      "[CV 4/5] END boosting_type=goss, n_estimators=100;, score=0.637 total time=   4.2s\n",
      "[CV 5/5] END boosting_type=goss, n_estimators=100;, score=0.639 total time=   4.1s\n",
      "Model: LGBMClassifier(boosting_type='goss',\n",
      "               class_weight={0: 0.5524624642168604, 1: 5.265311804008909},\n",
      "               random_state=42), make_scorer(f1_score, average=macro): 0.6384900218944738\n",
      "elapsed time: 53.982136487960815 s \n",
      "\n",
      "Best threshold is: 0.34, with score: 0.7308314023042528\n",
      "AUC score: 0.8734199814395895, F1-Macro: 0.7308314023042528\n",
      "Model: LGBMClassifier, Score: 0.7308314023042528\n",
      "\n",
      "               precision    recall  f1-score       support\n",
      "Non-Toxic      0.942593  0.969379  0.955798  39940.000000\n",
      "Toxic          0.599804  0.437366  0.505864   4191.000000\n",
      "accuracy       0.918855  0.918855  0.918855      0.918855\n",
      "macro avg      0.771198  0.703372  0.730831  44131.000000\n",
      "weighted avg   0.910039  0.918855  0.913069  44131.000000\n"
     ]
    }
   ],
   "source": [
    "best_model = main(x_train_vect, y_train, x_valid_vect, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf733bb-59fe-481d-8abb-6a40759643aa",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "To create prediction function, first, you must know how the data will be passed to the predictor. Often, it requires agreement from your Backend Engineer, MLOps Engineer, and Project manager.\n",
    "\n",
    "Let's assume that the data will be predicted one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78c0b1f3-bf94-4c80-b293-b6085b6762a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous preprocessing, we work with DataFrame.\n",
    "# It'll be easier for us to also work with DataFrame in the prediction stage\n",
    "\n",
    "def df_constructor(text, id=0):\n",
    "    df = pd.DataFrame(data={'id':[id], 'comment_text':[text]})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69be98d9-ca59-41ab-9474-127350d5a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_predict(text, tfidf_vectorizer, model, threshold, id=0):\n",
    "    df = df_constructor(text, id)\n",
    "    df_preprocessed = preprocess(df)\n",
    "    df_vect, _ = vectorize_tfidf(df_preprocessed, tfidf_vectorizer)\n",
    "    \n",
    "    code2rel = {0: 'Non-Toxic', 1: 'Toxic'}\n",
    "    df_vect = df_vect.drop(columns='id')\n",
    "    proba = model.predict_proba(df_vect)[:, 1]\n",
    "    predict = 1 if proba > threshold else 0\n",
    "    \n",
    "    return code2rel[predict], proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03e8dd12-b7c7-42a5-a16b-4435f1ccefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = joblib.load(\"../output/vectorizer.pkl\")\n",
    "model = joblib.load('../output/mantab_model.pkl')\n",
    "threshold = joblib.load('../output/threshold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e251e43-860e-46d9-8390-3d13da90e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ghifa\\documents\\pacmann\\simple-ml-project-example\\projectenv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "text = \"fuck fuck fuck you cunt, you can't do anything but whining, you useless trash piece of shit.\"\n",
    "predict, proba = main_predict(text, tfidf_vect, model, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a9e8903-0fb5-4ab6-8664-1ee965d29aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toxic'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684e38f-04d4-44ff-942a-78b80a23c772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b68aad-6c47-4c92-826d-0bec789303c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae6b2d-41e6-47dd-be12-5e85ff73ca9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
